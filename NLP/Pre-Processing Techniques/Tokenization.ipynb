{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =\"\"\"Tokenization is the process of breaking a text into meaningful units, often called tokens. \n",
    "        These tokens could be words, phrases, or even individual characters, depending on the approach.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentence Tokenizer\n",
    "\n",
    "sent_token = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'a', 'text', 'into', 'meaningful', 'units', ',', 'often', 'called', 'tokens', '.', 'These', 'tokens', 'could', 'be', 'words', ',', 'phrases', ',', 'or', 'even', 'individual', 'characters', ',', 'depending', 'on', 'the', 'approach', '.']\n"
     ]
    }
   ],
   "source": [
    "## word Tokenizer\n",
    "\n",
    "print(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"running\", \"jumps\", \"easily\", \"happily\", \"studies\", \"cats\",\n",
    "    \"went\", \"better\", \n",
    "    \"arguing\", \"agreement\", \"connection\", \"flying\",\n",
    "    \"programming\", \"tokenizer\", \"automated\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running------>run\n",
      "jumps------>jump\n",
      "easily------>easili\n",
      "happily------>happili\n",
      "studies------>studi\n",
      "cats------>cat\n",
      "went------>went\n",
      "better------>better\n",
      "arguing------>argu\n",
      "agreement------>agreement\n",
      "connection------>connect\n",
      "flying------>fli\n",
      "programming------>program\n",
      "tokenizer------>token\n",
      "automated------>autom\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"------>\"+ stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
