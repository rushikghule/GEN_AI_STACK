{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU SCORE\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is a metric that measures the accuracy of machine translations by comparing them to human reference translations using n-gram precision and a brevity penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "blue = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_text = [\"They cancelled the match because it was raining \"]\n",
    "reference_text = [\"They cancelled the match because of bad weather\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = blue.compute(predictions=prediction_text, references= reference_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.5169731539571706,\n",
       " 'precisions': [0.625, 0.5714285714285714, 0.5, 0.4],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0,\n",
       " 'translation_length': 8,\n",
       " 'reference_length': 8}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Score Limitations\n",
    "\n",
    "No semantic understanding – It treats synonyms (\"called off\" vs. \"cancelled\") as different words. \n",
    "Ignores word importance – Gives equal weight to all words, even less meaningful ones like prepositions.\n",
    "Limited word order preservation – Captures short sequences but not long sentence structure.\n",
    "Only exact matches – Doesn’t recognize variations (\"rain\" vs. \"raining\").\n",
    "Precision-focused – Checks correctness but not completeness of translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE SCORE\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate text summarization and machine translation by measuring n-gram overlap between machine-generated and reference texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_text = [\"They cancelled the match because it was raining \"]\n",
    "reference_text = [\"They cancelled the match because of bad weather\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.625,\n",
       " 'rouge2': 0.5714285714285714,\n",
       " 'rougeL': 0.625,\n",
       " 'rougeLsum': 0.625}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(predictions=prediction_text, references=reference_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Score Limitations\n",
    "No semantic understanding – It doesn’t recognize synonyms (\"huge\" vs. \"massive\").\n",
    "Limited word order detection – Struggles with sentence structure, especially for short n-grams.\n",
    "No length penalty – Can’t penalize summaries that are too short or contain extra details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METEOR SCORE\n",
    "\n",
    "METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a machine translation evaluation metric that improves upon BLEU by considering synonyms, stemming (word variations), and word order in its scoring.\n",
    "\n",
    "Recognizes synonyms (happy vs. joyful)\n",
    "\n",
    "Handles word variations (run vs. running)\n",
    "\n",
    "Considers word order (penalizes incorrect structure)\n",
    "\n",
    "Balances precision & recall, unlike BLEU, which mainly focuses on precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d94352d61c4aca9d9e4fa56ee21973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91880\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91880\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\91880\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "meteor = evaluate.load('meteor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\n",
    "references = [\"It is a guide to action that ensures that the military will forever heed Party commands\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = meteor.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meteor': 0.6944444444444445}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
